#Lauching spark under YARN
pyspark --master yarn --conf spark.ui.port=12901

#Spark Context
>>> orders = sc.textFile('/public/retail_db/orders')
>>> type(orders)
<class 'pyspark.rdd.RDD'>

# sample Row Level transformation(map, filter, flatMap), aggreations, ranking, joins, 
Maporders = orders.map(lambda order: (int(order.split(',')[0]), order.split(',')[1]))

#action e.g count, reduce, writing into files
orders.count()


#Spark modules (CORE, Spark SQL and Data FRames, ML Pipelines, Graph X Pipelines, Structured Streaming)

##Spark Data Structures
RDD
Data Frames = act same as tables in dbs, RDD with a structcture
RDD = low level data structure  which spark uses to distrbute data between tasks while the data is being processed

start spark =  pyspark --master yarn --conf spark.ui.port=12901
#create object in scala
data = sc.textFile('public/randomtextwriter/part-m-00000')
data.count()

##after the above operations rdds will be created
data = sc.textFile('public/randomtextwriter/part-m-00000', minPartitions=18,)

#read data 
data = spark.read.text('public/randomtextwriter/part-m-00000')
type(data)
#shows the schema
data.printSchema()
#data countt
data.count()

--above used to read data from file systems (S3, HDFS, AzureBlob, Local files systems)
--different APIs to read data from different data types

#develop a word count program to read data from public/randomtextwriter/part-m-00000' and save output to '/user/training'
#method 1
>>> data = sc.textFile('/public/randomtextwriter/part-m-00000')
>>> type(data)
<class 'pyspark.rdd.RDD'>
>>> wc = data.\
... flatMap(lambda line: line.split(' ')).\
... map(lambda word: (word, 1)). \
... reduceByKey(lambda x,y: x + y)
>>> type(wc)
<class 'pyspark.rdd.PipelinedRDD'>
>>> for i in wc.take(10): print(i)

>>> wc. \
... map(lambda rec: rec[0] + ',' + str(rec[1])). \
... saveAsTextFile('/user/training/core/wordcount')


#method 2
>>> data = spark.read.text('/public/randomtextwriter/part-m-00000')
>>> from pyspark.sql.functions import split, explode
>>> wc1 = data. \
... select(explode(split(data.value, ' ')).alias('words)). \
... groupBy('words'). \
... count()

>>> wc1.show
>>> wc1.write.csv('/user/training/df/wordcount/')
